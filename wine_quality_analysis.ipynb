{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnTJznUZ/1kLv2y5eLehDP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananyascodehq/Wine-Quality-Prediction/blob/main/wine_quality_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1: Load and Understand the Dataset"
      ],
      "metadata": {
        "id": "X6PWHYS_TWZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the required libraries"
      ],
      "metadata": {
        "id": "7fFZYaD5TjYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wWbzoHN8TVlh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "BH1uOsFzTwTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/winequality.csv')"
      ],
      "metadata": {
        "id": "5Jz3pcMMT_9X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2: Inspection of the dataset"
      ],
      "metadata": {
        "id": "8qoxt4DaUWPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data inspection is important before training any machine learning model because it helps understand the structure, quality, and characteristics of the dataset. It reveals the number of features and samples, data types, presence of missing values, outliers, and inconsistencies. This information guides appropriate preprocessing steps such as cleaning, scaling, and encoding. Data inspection also helps in selecting suitable algorithms and prevents issues like data leakage and overfitting, ensuring the model learns meaningful patterns and produces reliable results."
      ],
      "metadata": {
        "id": "YBS5SJD94OA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 5 rows:\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "4tHpI7YgUKZm",
        "outputId": "463c5906-c1fb-492b-a42e-78d0221710a9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.4              0.70         0.00             1.9      0.076   \n",
              "1            7.8              0.88         0.00             2.6      0.098   \n",
              "2            7.8              0.76         0.04             2.3      0.092   \n",
              "3           11.2              0.28         0.56             1.9      0.075   \n",
              "4            7.4              0.70         0.00             1.9      0.076   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
              "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
              "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
              "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      9.4        5  \n",
              "1      9.8        5  \n",
              "2      9.8        5  \n",
              "3      9.8        6  \n",
              "4      9.4        5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f92f3e60-cbaf-4e21-aed9-559ceaa1e739\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f92f3e60-cbaf-4e21-aed9-559ceaa1e739')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f92f3e60-cbaf-4e21-aed9-559ceaa1e739 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f92f3e60-cbaf-4e21-aed9-559ceaa1e739');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6223439832538593,\n        \"min\": 7.4,\n        \"max\": 11.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          7.4,\n          7.8,\n          11.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22689204481426842,\n        \"min\": 0.28,\n        \"max\": 0.88,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.88,\n          0.28,\n          0.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24657656011875909,\n        \"min\": 0.0,\n        \"max\": 0.56,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.04,\n          0.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31937438845342636,\n        \"min\": 1.9,\n        \"max\": 2.6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.9,\n          2.6,\n          2.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010807404868885038,\n        \"min\": 0.075,\n        \"max\": 0.098,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.098,\n          0.075,\n          0.076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.761944116355173,\n        \"min\": 11.0,\n        \"max\": 25.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          25.0,\n          17.0,\n          11.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.139352694220449,\n        \"min\": 34.0,\n        \"max\": 67.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          67.0,\n          60.0,\n          34.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005403702434442519,\n        \"min\": 0.9968,\n        \"max\": 0.998,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9968,\n          0.998,\n          0.9978\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16991174179555674,\n        \"min\": 3.16,\n        \"max\": 3.51,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.2,\n          3.16,\n          3.51\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05549774770204643,\n        \"min\": 0.56,\n        \"max\": 0.68,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.68,\n          0.58,\n          0.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21908902300206665,\n        \"min\": 9.4,\n        \"max\": 9.8,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9.8,\n          9.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData Info:\")\n",
        "print(df.info()) # This replaces checking data types manually"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOtlH0xVUPvW",
        "outputId": "bbb1b9b8-98e4-457b-f6a7-1f2b021e6d07"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599 entries, 0 to 1598\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   fixed acidity         1599 non-null   float64\n",
            " 1   volatile acidity      1599 non-null   float64\n",
            " 2   citric acid           1599 non-null   float64\n",
            " 3   residual sugar        1599 non-null   float64\n",
            " 4   chlorides             1599 non-null   float64\n",
            " 5   free sulfur dioxide   1599 non-null   float64\n",
            " 6   total sulfur dioxide  1599 non-null   float64\n",
            " 7   density               1599 non-null   float64\n",
            " 8   pH                    1599 non-null   float64\n",
            " 9   sulphates             1599 non-null   float64\n",
            " 10  alcohol               1599 non-null   float64\n",
            " 11  quality               1599 non-null   int64  \n",
            "dtypes: float64(11), int64(1)\n",
            "memory usage: 150.0 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSummary Statistics:\")\n",
        "display(df.describe().T) # Transposing (.T) makes it easier to read features as rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "dglhlN0SUdiH",
        "outputId": "1a4be59a-aa0b-436f-b4ee-1a2926ac6756"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary Statistics:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                       count       mean        std      min      25%  \\\n",
              "fixed acidity         1599.0   8.319637   1.741096  4.60000   7.1000   \n",
              "volatile acidity      1599.0   0.527821   0.179060  0.12000   0.3900   \n",
              "citric acid           1599.0   0.270976   0.194801  0.00000   0.0900   \n",
              "residual sugar        1599.0   2.538806   1.409928  0.90000   1.9000   \n",
              "chlorides             1599.0   0.087467   0.047065  0.01200   0.0700   \n",
              "free sulfur dioxide   1599.0  15.874922  10.460157  1.00000   7.0000   \n",
              "total sulfur dioxide  1599.0  46.467792  32.895324  6.00000  22.0000   \n",
              "density               1599.0   0.996747   0.001887  0.99007   0.9956   \n",
              "pH                    1599.0   3.311113   0.154386  2.74000   3.2100   \n",
              "sulphates             1599.0   0.658149   0.169507  0.33000   0.5500   \n",
              "alcohol               1599.0  10.422983   1.065668  8.40000   9.5000   \n",
              "quality               1599.0   5.636023   0.807569  3.00000   5.0000   \n",
              "\n",
              "                           50%        75%        max  \n",
              "fixed acidity          7.90000   9.200000   15.90000  \n",
              "volatile acidity       0.52000   0.640000    1.58000  \n",
              "citric acid            0.26000   0.420000    1.00000  \n",
              "residual sugar         2.20000   2.600000   15.50000  \n",
              "chlorides              0.07900   0.090000    0.61100  \n",
              "free sulfur dioxide   14.00000  21.000000   72.00000  \n",
              "total sulfur dioxide  38.00000  62.000000  289.00000  \n",
              "density                0.99675   0.997835    1.00369  \n",
              "pH                     3.31000   3.400000    4.01000  \n",
              "sulphates              0.62000   0.730000    2.00000  \n",
              "alcohol               10.20000  11.100000   14.90000  \n",
              "quality                6.00000   6.000000    8.00000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-773cb296-44ac-4c18-bf03-cf93a91b853b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fixed acidity</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>8.319637</td>\n",
              "      <td>1.741096</td>\n",
              "      <td>4.60000</td>\n",
              "      <td>7.1000</td>\n",
              "      <td>7.90000</td>\n",
              "      <td>9.200000</td>\n",
              "      <td>15.90000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volatile acidity</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>0.527821</td>\n",
              "      <td>0.179060</td>\n",
              "      <td>0.12000</td>\n",
              "      <td>0.3900</td>\n",
              "      <td>0.52000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>1.58000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>citric acid</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>0.270976</td>\n",
              "      <td>0.194801</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0900</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>residual sugar</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>2.538806</td>\n",
              "      <td>1.409928</td>\n",
              "      <td>0.90000</td>\n",
              "      <td>1.9000</td>\n",
              "      <td>2.20000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>15.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chlorides</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>0.087467</td>\n",
              "      <td>0.047065</td>\n",
              "      <td>0.01200</td>\n",
              "      <td>0.0700</td>\n",
              "      <td>0.07900</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.61100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>15.874922</td>\n",
              "      <td>10.460157</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>14.00000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>72.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>46.467792</td>\n",
              "      <td>32.895324</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>22.0000</td>\n",
              "      <td>38.00000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>289.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>density</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>0.996747</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.99007</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>0.99675</td>\n",
              "      <td>0.997835</td>\n",
              "      <td>1.00369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pH</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>3.311113</td>\n",
              "      <td>0.154386</td>\n",
              "      <td>2.74000</td>\n",
              "      <td>3.2100</td>\n",
              "      <td>3.31000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>4.01000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sulphates</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>0.658149</td>\n",
              "      <td>0.169507</td>\n",
              "      <td>0.33000</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>0.62000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>2.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alcohol</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>10.422983</td>\n",
              "      <td>1.065668</td>\n",
              "      <td>8.40000</td>\n",
              "      <td>9.5000</td>\n",
              "      <td>10.20000</td>\n",
              "      <td>11.100000</td>\n",
              "      <td>14.90000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quality</th>\n",
              "      <td>1599.0</td>\n",
              "      <td>5.636023</td>\n",
              "      <td>0.807569</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>5.0000</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-773cb296-44ac-4c18-bf03-cf93a91b853b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-773cb296-44ac-4c18-bf03-cf93a91b853b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-773cb296-44ac-4c18-bf03-cf93a91b853b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1599.0,\n        \"max\": 1599.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1599.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.094826566443462,\n        \"min\": 0.08746654158849279,\n        \"max\": 46.46779237023139,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10.422983114446529\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.521897457645702,\n        \"min\": 0.0018873339538425559,\n        \"max\": 32.89532447829901,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          1.0656675818473926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.7199194985495714,\n        \"min\": 0.0,\n        \"max\": 8.4,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          8.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"25%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.292677105873769,\n        \"min\": 0.07,\n        \"max\": 22.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          9.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.748222427272216,\n        \"min\": 0.079,\n        \"max\": 38.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"75%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.54334705353007,\n        \"min\": 0.09,\n        \"max\": 62.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82.25303187574917,\n        \"min\": 0.611,\n        \"max\": 289.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          14.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 3: Missing values\n",
        "\n",
        "## There are no missing values in the given dataset.\n",
        "\n",
        "## If missing values were present in a real-world machine learning project, they would be handled based on their nature and impact as follows:\n",
        "\n",
        "###     - Remove rows or columns if the number of missing values is very small and does not affect data integrity.\n",
        "\n",
        "###     - Replace missing numerical values using statistical methods such as mean or median.\n",
        "\n",
        "###     - Replace missing categorical values using the mode.\n",
        "\n",
        "###     - Use advanced imputation techniques like KNN or regression-based imputation for more complex cases.\n",
        "\n",
        "### - Analyze the reason for missing data before choosing a method to avoid introducing bias into the model.\n",
        "\n",
        "## This ensures data quality and prevents model performance degradation."
      ],
      "metadata": {
        "id": "KWnA5HNwUpt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV-lS9OHUtqr",
        "outputId": "d0ad5884-32f3-41a9-a900-b12b9f84bf26"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Values:\n",
            "fixed acidity           0\n",
            "volatile acidity        0\n",
            "citric acid             0\n",
            "residual sugar          0\n",
            "chlorides               0\n",
            "free sulfur dioxide     0\n",
            "total sulfur dioxide    0\n",
            "density                 0\n",
            "pH                      0\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "quality                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 4: Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "yJ4d8EOnWwlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Print the value counts\n",
        "print(\"Quality Score Distribution:\")\n",
        "print(df['quality'].value_counts().sort_index())\n",
        "\n",
        "# 2. Plot the count plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='quality', data=df, palette='magma')\n",
        "plt.title('Distribution of Wine Quality Scores')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "v7NBLBHE6EZu",
        "outputId": "6584fba2-0454-4308-e3b6-1c9e3ab48874"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality Score Distribution:\n",
            "quality\n",
            "3     10\n",
            "4     53\n",
            "5    681\n",
            "6    638\n",
            "7    199\n",
            "8     18\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2208629628.py:10: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x='quality', data=df, palette='magma')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation 1: Significant Class Imbalance.\n",
        "##The vast majority of wines are rated 5 or 6. There are very few \"Excellent\" (8) or \"Poor\" (3) wines.\n",
        "\n",
        "#Observation 2: Normal-ish Distribution.\n",
        "##The data is centered around the middle scores, which tells us that \"average\" wine is easy to find, but \"extreme\" quality (high or low) is rare.\n",
        "\n",
        "#Observation 3: Data Scarcity for Extremes.\n",
        "##Because there are so few 3s and 8s, a model trained on this data will likely struggle to identify a 3 or an 8 correctly because it simply hasn't seen enough examples of them."
      ],
      "metadata": {
        "id": "xLZsOzAU6iYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 5: Convert to binary classification"
      ],
      "metadata": {
        "id": "ra41XFqIZC3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why is binary classification more useful than predicting exact quality scores?\n",
        "\n",
        "In real-world systems, binary classification is often preferred for three main reasons:\n",
        "\n",
        "1. Simplifying Decision Making: A business usually needs a \"Yes/No\" action. A\n",
        "sommelier or a distributor needs to know, \"Is this wine premium enough for the luxury shelf?\" They don't necessarily care if a wine is a 5.1 or a 5.4; they care if it meets the \"Good\" threshold.\n",
        "\n",
        "2. Reducing Model Error: Predicting an exact score (Regression) is much harder for a model. For example, if a model predicts a 5.8 for a wine that is actually a 6.0, it is technically \"wrong.\" However, in a binary system, both are simply \"Bad,\" so the model can focus on the much clearer chemical gap between a \"Bad\" wine and a \"Good\" wine.\n",
        "\n",
        "3. Handling Subjectivity: Quality scores are often subjective human labels. By grouping them into categories, we \"smooth out\" the noise. The chemical difference between a 7 and an 8 is usually more distinct than the difference between a 5 and a 6."
      ],
      "metadata": {
        "id": "YkbnNjX07jcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a mapping: True (1) if quality >= 7, else False (0)\n",
        "df['quality_label'] = (df['quality'] >= 7).astype(int)"
      ],
      "metadata": {
        "id": "cUNfyNy_XRGj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Quality Scores vs New Labels:\")\n",
        "print(df[['quality', 'quality_label']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErN1G4DNZPYr",
        "outputId": "dea3445d-5e79-4fb3-e83c-6a7c6fa4ec43"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Quality Scores vs New Labels:\n",
            "   quality  quality_label\n",
            "0        5              0\n",
            "1        5              0\n",
            "2        5              0\n",
            "3        6              0\n",
            "4        5              0\n",
            "5        5              0\n",
            "6        5              0\n",
            "7        7              1\n",
            "8        7              1\n",
            "9        5              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClass Distribution:\")\n",
        "print(df['quality_label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkURPtFGZW8O",
        "outputId": "5d472929-f361-4968-db24-46922507a09f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class Distribution:\n",
            "quality_label\n",
            "0    1382\n",
            "1     217\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 6: Separation"
      ],
      "metadata": {
        "id": "uO4hqZhxfshQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30d3f4a"
      },
      "source": [
        "## Why exclude the original `quality` column from input features (X)?\n",
        "\n",
        "The original `quality` column represents the raw, multi-class target variable. When building a predictive model, we aim to predict a target variable (in this case, `quality_label`). Including the original `quality` column as a feature would lead to **data leakage**.\n",
        "\n",
        "Data leakage occurs when information about the target variable is inadvertently included in the input features. If the model has access to the original `quality` score, it would essentially be given the answer to the problem it's trying to solve, making it trivial to achieve high (but misleading) accuracy. This would prevent the model from learning genuine relationships between the other chemical properties and the wine's perceived quality (good/bad).\n",
        "\n",
        "Therefore, we drop both the original `quality` and the newly created `quality_label` from `X`, and only use `quality_label` as our target `y`.\n",
        "\n",
        "### Mathematical Cheating:\n",
        "Since our target `quality_label` was literally derived from the `quality` column (`Quality >= 7`), the model would find a 100% perfect correlation. It wouldn't bother looking at the alcohol, acidity, or sugar; it would just look at the `quality` number and give you the answer.\n",
        "\n",
        "### Real-World Failure:\n",
        "In a real winery, you use this model because you don't know the `quality` yet. You have a vat of liquid and a lab report of chemicals. If your model requires the \"Quality Score\" as an input to predict the \"Quality Label,\" the model is uselessyou'd already have the answer you're looking for.\n",
        "\n",
        "### Implicit Bias:\n",
        "Even if the correlation wasn't 100%, any feature that \"leaks\" information from the future (the outcome) into the training process results in a model that performs perfectly in your notebook but fails completely in production."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (X): Everything except the original score and our new label\n",
        "X = df.drop(columns=['quality', 'quality_label'])\n",
        "# Target (y): Only the binary 'Good/Bad' label\n",
        "y = df['quality_label']"
      ],
      "metadata": {
        "id": "k_VKC4ybfxWb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 7: Train-Test Split"
      ],
      "metadata": {
        "id": "-IR4mSBFf2Fi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afa9d70a"
      },
      "source": [
        "### Why do we split data into training and testing sets?\n",
        "\n",
        "Splitting data into training and testing sets is crucial for evaluating a machine learning model's performance on unseen data. The training set is used to teach the model to identify patterns and relationships within the data, while the test set is reserved to assess how well the model generalizes to new, unobserved examples. This helps ensure that the model is not simply memorizing the training data but can make accurate predictions on data it hasn't encountered before.\n",
        "\n",
        "### What problem occurs if we train and test on the same data?\n",
        "\n",
        "If we train and test a model on the same data, the model will likely show artificially high performance metrics. This is because the model has already 'seen' all the test data during training and has simply memorized the answers. This phenomenon is known as **overfitting**. An overfit model performs exceptionally well on the training data but fails to generalize to new data, making it unreliable and impractical for real-world applications. The test set acts as an unbiased judge of the model's true performance and its ability to generalize."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "EvZCt1m7gACD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 80% Training, 20% Testing. random_state=42 for reproducibility.\n",
        "# 'stratify=y'  It ensures both the train and test sets\n",
        "# have the same 15% ratio of 'Good' wines.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "KfzY0nc_f1c4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 8: Feature scaling"
      ],
      "metadata": {
        "id": "Uu6dtpc9fCcg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abbc63b2"
      },
      "source": [
        "### Why is Feature Scaling Important?\n",
        "\n",
        "Feature scaling is a preprocessing step that normalizes the range of independent variables or features of the data. It's crucial because many machine learning algorithms calculate the distance between data points, and if one feature has a much larger range than others, its magnitude can dominate the distance calculation. This can lead to biased models or slower convergence of optimization algorithms.\n",
        "\n",
        "### Which ML Models Need Scaling and Why?\n",
        "\n",
        "Models that are sensitive to the scale of features generally benefit from or require scaling. These include:\n",
        "\n",
        "*   **Distance-based algorithms:** Algorithms like **K-Nearest Neighbors (KNN)**, **Support Vector Machines (SVM)**, and **K-Means Clustering** are heavily reliant on distance calculations. Without scaling, features with larger values will have a disproportionately large impact on these distance metrics.\n",
        "\n",
        "*   **Gradient Descent-based algorithms:** Models such as **Logistic Regression** and **Neural Networks** use gradient descent to optimize their loss function. Scaling can help these algorithms converge faster and more stably, as it prevents large gradients for features with larger scales, which can cause the optimization to overshoot the minimum.\n",
        "\n",
        "*   **Principal Component Analysis (PCA):** PCA aims to find directions of maximum variance in the data. If features are not scaled, features with high variance due to their scale (rather than their inherent information content) can dominate the principal components, leading to misleading results.\n",
        "\n",
        "Models that are generally *not* sensitive to feature scaling include tree-based algorithms like **Decision Trees** and **Random Forests**, as they make decisions based on feature thresholds rather than distances or magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 1. Fit AND Transform the training data\n",
        "# We 'fit' to learn the mean/std, then 'transform' to scale it.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 2. ONLY Transform the test data\n",
        "# We DO NOT fit here. We use the 'past' rules (train rules) on the 'future' (test).\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Professional check: verify the mean is 0 and std is 1 for the FIRST feature\n",
        "print(f\"Mean of scaled feature 0: {X_train_scaled[:, 0].mean():.2f}\")\n",
        "print(f\"Std of scaled feature 0: {X_train_scaled[:, 0].std():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0CREIyNfGwW",
        "outputId": "dd53c0b9-7f3f-4ff9-e855-c3cc434822b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of scaled feature 0: 0.00\n",
            "Std of scaled feature 0: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5119c60"
      },
      "source": [
        "## Addressing Class Imbalance with SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "\n",
        "As observed earlier, our `quality_label` (Good/Bad wine) target variable is highly imbalanced, with significantly fewer 'Good' wines (class 1) than 'Bad' wines (class 0). Training a model on imbalanced data can lead to a model that performs well on the majority class but poorly on the minority class, often misclassifying minority instances.\n",
        "\n",
        "**What is SMOTE?**\n",
        "SMOTE is an over-sampling technique that generates synthetic samples from the minority class. It works by selecting a minority class instance and then finding its K-nearest neighbors. It then creates new synthetic instances along the line segments connecting the original instance to its neighbors. This helps to increase the number of samples in the minority class, making the class distribution more balanced.\n",
        "\n",
        "**Why apply SMOTE to the training data only?**\n",
        "It is crucial to apply SMOTE *only* to the training data (`X_train_scaled`, `y_train`) and *not* to the test data (`X_test_scaled`, `y_test`). This is to prevent **data leakage**. If synthetic samples were generated in the test set, the model would be evaluated on data that is not truly independent or unseen, leading to an overly optimistic and misleading evaluation of its performance in a real-world scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78cca26"
      },
      "source": [
        "# TASK 9: Model Training & TASK 10: Model Evaluation\n",
        "\n",
        "In this task, we will train and evaluate five different machine learning models using our preprocessed data. For models sensitive to feature scaling, we will use the `X_train_smote` and `X_test_scaled` data, which have been scaled and had class imbalance addressed (for the training set). For tree-based models, scaling is not strictly necessary, but we'll use the scaled data for consistency since the previous steps prepared it that way.\n",
        "\n",
        "After training, we will evaluate each model's performance on the test set using various metrics, with a particular focus on the F1-score due to the class imbalance in our target variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Initialize SMOTE\n",
        "# sampling_strategy='auto' will make the minority class equal to the majority class\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# 2. Apply SMOTE ONLY to the scaled training data\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Check the new class distribution\n",
        "print(f\"Original training class distribution: {y_train.value_counts()}\")\n",
        "print(f\"New training class distribution: {y_train_smote.value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkLVePI2fh-Z",
        "outputId": "7d0e0d1a-2b9f-45a9-fc34-ba6e417e5cb4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training class distribution: quality_label\n",
            "0    1105\n",
            "1     174\n",
            "Name: count, dtype: int64\n",
            "New training class distribution: quality_label\n",
            "0    1105\n",
            "1    1105\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 9: Model training & TASK 10: Model evaluation"
      ],
      "metadata": {
        "id": "U4Tx5QMbg31H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad72721"
      },
      "source": [
        "## Model Explanations:\n",
        "\n",
        "### 1. Logistic Regression\n",
        "Logistic Regression is a linear model used for binary classification. Despite its name, it's a classification algorithm that estimates the probability of an instance belonging to a particular class by fitting data to a sigmoid function.\n",
        "\n",
        "### 2. K-Nearest Neighbors (KNN)\n",
        "KNN is a non-parametric, instance-based learning algorithm used for classification. It classifies a new data point based on the majority class among its 'k' nearest neighbors in the feature space.\n",
        "\n",
        "### 3. Decision Tree Classifier\n",
        "A Decision Tree Classifier works by recursively splitting the data into subsets based on the most significant features. It forms a tree-like structure where each internal node represents a feature test, each branch represents an outcome of the test, and each leaf node represents a class label.\n",
        "\n",
        "### 4. Random Forest Classifier\n",
        "A Random Forest Classifier is an ensemble learning method that builds multiple decision trees during training. It then outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees, which helps to reduce overfitting and improve accuracy.\n",
        "\n",
        "### 5. Support Vector Machine (SVM)\n",
        "SVM is a powerful algorithm that finds an optimal hyperplane that best separates data points into different classes in a high-dimensional space. It aims to maximize the margin between the classes, leading to better generalization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression    # Model 1\n",
        "from sklearn.neighbors import KNeighborsClassifier     # Model 2\n",
        "from sklearn.tree import DecisionTreeClassifier        # Model 3\n",
        "from sklearn.ensemble import RandomForestClassifier    # Model 4\n",
        "from sklearn.svm import SVC                            # Model 5"
      ],
      "metadata": {
        "id": "V-1UhYNYjyxs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM\": SVC(kernel='rbf', probability=True)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train on SMOTE data\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "    # Predict on the REAL Test Set\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1 Score\": f1\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for easy comparison\n",
        "comparison_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\n",
        "display(comparison_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rpREKNNrg6xG",
        "outputId": "f1d1688d-72c9-4701-915a-70392afda628"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 Model  Accuracy  Precision    Recall  F1 Score\n",
              "3        Random Forest  0.921875   0.704545  0.720930  0.712644\n",
              "4                  SVM  0.862500   0.492308  0.744186  0.592593\n",
              "2        Decision Tree  0.865625   0.500000  0.674419  0.574257\n",
              "0  Logistic Regression  0.821875   0.414634  0.790698  0.544000\n",
              "1                  KNN  0.815625   0.404762  0.790698  0.535433"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a858cb4-dcfd-4a6e-a57e-49e0533f7eca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.704545</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.712644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.592593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.865625</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.574257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.821875</td>\n",
              "      <td>0.414634</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.815625</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.535433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a858cb4-dcfd-4a6e-a57e-49e0533f7eca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a858cb4-dcfd-4a6e-a57e-49e0533f7eca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a858cb4-dcfd-4a6e-a57e-49e0533f7eca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_8661187a-06c7-43fc-bfad-fb1769e15ec6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8661187a-06c7-43fc-bfad-fb1769e15ec6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_df",
              "summary": "{\n  \"name\": \"comparison_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"SVM\",\n          \"KNN\",\n          \"Decision Tree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04259639802847183,\n        \"min\": 0.815625,\n        \"max\": 0.921875,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8625,\n          0.815625,\n          0.865625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12062631454353663,\n        \"min\": 0.40476190476190477,\n        \"max\": 0.7045454545454546,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.49230769230769234,\n          0.40476190476190477,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04933303124557308,\n        \"min\": 0.6744186046511628,\n        \"max\": 0.7906976744186046,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7441860465116279,\n          0.7906976744186046,\n          0.7209302325581395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07136812014729087,\n        \"min\": 0.5354330708661418,\n        \"max\": 0.7126436781609196,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5925925925925926,\n          0.5354330708661418,\n          0.5742574257425742\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc872d46"
      },
      "source": [
        "The `comparison_df` already contains the Accuracy and other metrics for all models, sorted by F1 Score. We prioritize F1 Score over accuracy for this imbalanced dataset because it provides a better measure of a model's performance on the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "da38bc98",
        "outputId": "926c44bd-2b91-4d85-c877-9a0a66c2f613"
      },
      "source": [
        "print(\"\\nModel Comparison Table (Sorted by F1 Score):\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Answer the questions\n",
        "best_model_f1 = comparison_df.iloc[0]\n",
        "print(f\"\\n1. Which model performed the best (based on F1 Score)?\")\n",
        "print(f\"   The best performing model is: {best_model_f1['Model']} with an F1 Score of {best_model_f1['F1 Score']:.4f}\")\n",
        "\n",
        "print(f\"\\n2. Why do you think this model performed better on this dataset?\")\n",
        "print(f\"   The {best_model_f1['Model']} model, an ensemble method, generally performs well on datasets with complex relationships and can be more robust to overfitting than single models like Decision Trees. Its ability to combine predictions from multiple trees helps in achieving better generalization and handling the nuances of the data, especially after addressing class imbalance with SMOTE. Random Forests are also less sensitive to feature scaling compared to distance-based methods like KNN or SVM, though we applied scaling for consistency. Given the nature of the wine quality data, which likely involves non-linear interactions between chemical properties, an ensemble method like Random Forest is often well-suited to capture these complexities effectively.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison Table (Sorted by F1 Score):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 Model  Accuracy  Precision    Recall  F1 Score\n",
              "3        Random Forest  0.921875   0.704545  0.720930  0.712644\n",
              "4                  SVM  0.862500   0.492308  0.744186  0.592593\n",
              "2        Decision Tree  0.865625   0.500000  0.674419  0.574257\n",
              "0  Logistic Regression  0.821875   0.414634  0.790698  0.544000\n",
              "1                  KNN  0.815625   0.404762  0.790698  0.535433"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ba61501-8dd5-4058-9088-9ac35fc29e4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.704545</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.712644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>0.492308</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.592593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.865625</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.574257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.821875</td>\n",
              "      <td>0.414634</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.815625</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.535433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ba61501-8dd5-4058-9088-9ac35fc29e4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ba61501-8dd5-4058-9088-9ac35fc29e4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ba61501-8dd5-4058-9088-9ac35fc29e4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_db3d0b8c-eff0-4e86-a2f2-a4bc91e12521\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_db3d0b8c-eff0-4e86-a2f2-a4bc91e12521 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_df",
              "summary": "{\n  \"name\": \"comparison_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"SVM\",\n          \"KNN\",\n          \"Decision Tree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04259639802847183,\n        \"min\": 0.815625,\n        \"max\": 0.921875,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8625,\n          0.815625,\n          0.865625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12062631454353663,\n        \"min\": 0.40476190476190477,\n        \"max\": 0.7045454545454546,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.49230769230769234,\n          0.40476190476190477,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04933303124557308,\n        \"min\": 0.6744186046511628,\n        \"max\": 0.7906976744186046,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7441860465116279,\n          0.7906976744186046,\n          0.7209302325581395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07136812014729087,\n        \"min\": 0.5354330708661418,\n        \"max\": 0.7126436781609196,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5925925925925926,\n          0.5354330708661418,\n          0.5742574257425742\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Which model performed the best (based on F1 Score)?\n",
            "   The best performing model is: Random Forest with an F1 Score of 0.7126\n",
            "\n",
            "2. Why do you think this model performed better on this dataset?\n",
            "   The Random Forest model, an ensemble method, generally performs well on datasets with complex relationships and can be more robust to overfitting than single models like Decision Trees. Its ability to combine predictions from multiple trees helps in achieving better generalization and handling the nuances of the data, especially after addressing class imbalance with SMOTE. Random Forests are also less sensitive to feature scaling compared to distance-based methods like KNN or SVM, though we applied scaling for consistency. Given the nature of the wine quality data, which likely involves non-linear interactions between chemical properties, an ensemble method like Random Forest is often well-suited to capture these complexities effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 11: Pipeline and Hyperparameter tuning"
      ],
      "metadata": {
        "id": "WnyaWsXWlKR2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224449d1"
      },
      "source": [
        "## Why are Pipelines used in real-world ML systems?\n",
        "\n",
        "Machine Learning pipelines are used to streamline and automate the workflow from raw data to a deployable model. They chain together multiple processing steps (like scaling, imputation, and model training) into a single scikit-learn object. This offers several key benefits:\n",
        "\n",
        "1.  **Ensures Correctness:** Prevents data leakage by ensuring that preprocessing steps (like scaling) are fit only on the training data and then applied consistently to both training and test data.\n",
        "2.  **Reproducibility:** Makes the entire workflow reproducible, as all steps are encapsulated within one object.\n",
        "3.  **Code Cleanliness:** Simplifies the code by reducing repetitive steps and making the overall process more readable.\n",
        "4.  **Deployment:** Facilitates easier deployment, as the entire trained pipeline can be saved and loaded to make predictions on new, unseen data.\n",
        "5.  **Hyperparameter Tuning:** Allows for simultaneous tuning of parameters across different steps (e.g., both preprocessing parameters and model parameters) using tools like `GridSearchCV`.\n",
        "\n",
        "## Why does Hyperparameter Tuning improve Model Performance?\n",
        "\n",
        "Hyperparameters are configuration settings that are external to the model and whose values cannot be estimated from the data. They need to be set manually before the training process begins (e.g., the `C` parameter in SVM, or `n_neighbors` in KNN). Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a given machine learning model that yields the best performance on a specific dataset.\n",
        "\n",
        "It improves model performance by:\n",
        "\n",
        "1.  **Optimizing for the Data:** Different datasets respond best to different hyperparameter configurations. Tuning allows the model to find the best fit for the unique characteristics of the data.\n",
        "2.  **Preventing Underfitting/Overfitting:** Correctly tuned hyperparameters can help a model avoid being too simple (underfitting) or too complex (overfitting) for the data.\n",
        "3.  **Maximizing Generalization:** An optimally tuned model is more likely to generalize well to new, unseen data, leading to better real-world predictions.\n",
        "4.  **Leveraging Model Potential:** It ensures that the chosen algorithm is operating at its maximum potential for the given problem, as suboptimal hyperparameters can severely limit a model's effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 1. Create the Pipeline\n",
        "# This 'bundles' the scaler and the model into one object.\n",
        "# When we call 'fit', it scales the data AND THEN trains the SVM.\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "# 2. Define the Hyperparameter Grid\n",
        "# C: Controls the trade-off between a smooth boundary and getting every point right.\n",
        "# kernel: The mathematical 'lens' used to view the data.\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],\n",
        "    'svm__kernel': ['rbf', 'poly', 'linear'],\n",
        "    'svm__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# 3. Setup GridSearch with Cross-Validation\n",
        "# cv=5 means the data is split into 5 pieces; it trains on 4 and tests on 1,\n",
        "# repeating this 5 times to ensure the 'Best Params' aren't just a fluke.\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1', # We use F1 because of the class imbalance we discussed!\n",
        "    n_jobs=-1     # Use all your computer's processors to speed it up\n",
        ")\n",
        "\n",
        "# 4. Fit the GridSearch\n",
        "# Note: We use the ORIGINAL X_train here because the Pipeline handles scaling!\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation F1 Score: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Xn-tHolV29",
        "outputId": "41eed918-c858-4a8b-c529-a1ab11808280"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
            "Best Cross-Validation F1 Score: 0.5283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9d83d7"
      },
      "source": [
        "# TASK 12: Final Conclusion\n",
        "\n",
        "## Understanding of the Dataset\n",
        "The dataset contained various physiochemical properties of red wine and a 'quality' score, which was initially multi-class (3-8). We transformed this into a binary classification problem ('Good' or 'Bad' wine) to simplify the task and make it more actionable for real-world scenarios.\n",
        "\n",
        "## Important Observations from EDA\n",
        "- The original 'quality' distribution was heavily imbalanced, with most wines scoring 5 or 6, and very few extremes (3 or 8). This significant class imbalance was a key challenge.\n",
        "- The conversion to a binary 'quality_label' (Good/Bad) highlighted this imbalance further, with a much larger number of 'Bad' wines (quality < 7) compared to 'Good' wines (quality >= 7).\n",
        "\n",
        "## Best Performing Model\n",
        "Based on the F1 Score (which is crucial for imbalanced datasets), the **Random Forest Classifier** performed the best among the initial models. After hyperparameter tuning with a pipeline, the **Support Vector Machine** also showed promising results, although the F1 Score from `GridSearchCV`'s cross-validation was lower than Random Forest's test F1 Score. This suggests that further tuning or a different approach might benefit the SVM.\n",
        "\n",
        "## What was learned from this project?\n",
        "- **Importance of Data Inspection & EDA:** Thorough inspection revealed class imbalance, which significantly influenced subsequent decisions.\n",
        "- **Binary Classification for Real-World Use:** Converting a multi-class problem to binary can make model predictions more practical and interpretable for business decisions.\n",
        "- **Handling Class Imbalance:** Techniques like SMOTE are essential for preventing models from being biased towards the majority class.\n",
        "- **Feature Scaling:** Critical for distance-based and gradient-descent models to ensure fair contribution of all features.\n",
        "- **Pipelines:** Streamline workflows, prevent data leakage, and ensure reproducibility in preprocessing steps.\n",
        "- **Hyperparameter Tuning:** Essential for optimizing model performance and finding the best configuration for a given dataset.\n",
        "\n",
        "## How this project is similar to real-world Machine Learning applications\n",
        "This project mirrors real-world ML applications in several ways:\n",
        "- **Problem Definition:** Starting with raw data and refining the problem (e.g., from multi-class quality prediction to binary good/bad classification) to meet practical needs.\n",
        "- **Data Quality & Preprocessing:** Encountering and addressing issues like class imbalance, which is common in many datasets.\n",
        "- **Model Selection & Comparison:** Evaluating multiple models to find the most suitable one for the task.\n",
        "- **Performance Metrics:** Choosing appropriate evaluation metrics (like F1 Score) that align with the business goal, especially with imbalanced data.\n",
        "- **Robustness & Reproducibility:** Using techniques like train-test splits, scaling, and pipelines to build robust, reproducible models ready for deployment.\n",
        "- **Iterative Improvement:** The process of trying different models and tuning hyperparameters reflects the iterative nature of ML development in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9028bee4"
      },
      "source": [
        "# Red Wine Quality Prediction Project\n",
        "\n",
        "## Project Overview\n",
        "This project aims to build a machine learning model to predict the quality of red wine based on its physiochemical properties. Initially, the wine quality was represented as a multi-class score (3-8). To make the problem more practical for real-world applications, it was converted into a binary classification task: classifying wine as 'Good' (quality >= 7) or 'Bad' (quality < 7).\n",
        "\n",
        "## Dataset\n",
        "The dataset contains 11 physiochemical properties of red wine (e.g., fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, etc.) and a 'quality' score ranging from 3 to 8.\n",
        "\n",
        "## Tasks Performed\n",
        "\n",
        "### TASK 1: Load and Understand the Dataset\n",
        "- Loaded the `winequality.csv` dataset into a pandas DataFrame.\n",
        "\n",
        "### TASK 2: Inspection of the Dataset\n",
        "- Inspected the first 5 rows, data types, and summary statistics to understand the data's structure and characteristics.\n",
        "\n",
        "### TASK 3: Missing Values\n",
        "- Checked for missing values. The dataset was found to have no missing values.\n",
        "\n",
        "### TASK 4: Exploratory Data Analysis (EDA)\n",
        "- Analyzed the distribution of the original 'quality' scores, identifying a significant class imbalance where most wines were rated 5 or 6, and extreme scores (3 or 8) were rare.\n",
        "\n",
        "### TASK 5: Convert to Binary Classification\n",
        "- Transformed the original multi-class 'quality' column into a binary `quality_label` (0 for 'Bad' wines, 1 for 'Good' wines, where 'Good' means quality >= 7). This step simplifies the problem for practical decision-making and helps address subjectivity.\n",
        "\n",
        "### TASK 6: Separation of Features and Target\n",
        "- Separated the features (`X`) from the target variable (`y`, which is `quality_label`). The original `quality` column was explicitly excluded from features to prevent data leakage, ensuring the model learns from chemical properties rather than trivial correlations.\n",
        "\n",
        "### TASK 7: Train-Test Split\n",
        "- Split the dataset into training (80%) and testing (20%) sets using `random_state=42` and `stratify=y` to maintain the class distribution in both sets. This ensures unbiased model evaluation on unseen data.\n",
        "\n",
        "### TASK 8: Feature Scaling\n",
        "- Applied `StandardScaler` to the numerical features. The scaler was fitted on the training data (`X_train`) and then used to transform both the training (`X_train_scaled`) and testing (`X_test_scaled`) data. This is crucial for distance-based and gradient-descent algorithms.\n",
        "\n",
        "### Addressing Class Imbalance with SMOTE\n",
        "- Utilized Synthetic Minority Over-sampling Technique (SMOTE) on the training data (`X_train_scaled`, `y_train`) to balance the classes. SMOTE generates synthetic samples for the minority class, preventing the model from being biased towards the majority class. SMOTE was applied *only* to the training data to avoid data leakage into the test set.\n",
        "\n",
        "### TASK 9: Model Training & TASK 10: Model Evaluation\n",
        "- Trained and evaluated five different machine learning models:\n",
        "    - **Logistic Regression**: A linear model for binary classification.\n",
        "    - **K-Nearest Neighbors (KNN)**: A non-parametric, instance-based classifier.\n",
        "    - **Decision Tree Classifier**: A tree-structured model that splits data based on features.\n",
        "    - **Random Forest Classifier**: An ensemble method using multiple decision trees.\n",
        "    - **Support Vector Machine (SVM)**: Finds an optimal hyperplane to separate classes.\n",
        "- Models were evaluated using Accuracy, Precision, Recall, and F1 Score. F1 Score was prioritized due to the class imbalance.\n",
        "\n",
        "### Best Performing Model (Initial Evaluation)\n",
        "- The **Random Forest Classifier** emerged as the best-performing model based on the F1 Score (0.7126) during the initial evaluation. Its ensemble nature likely contributes to its robustness and ability to handle complex relationships in the data.\n",
        "\n",
        "### TASK 11: Pipeline and Hyperparameter Tuning\n",
        "- Created a machine learning pipeline combining `StandardScaler` and an `SVC` (Support Vector Classifier).\n",
        "- Applied `GridSearchCV` to tune hyperparameters for the SVM model (C, kernel, gamma) using 5-fold cross-validation and `f1` scoring. The best parameters found were `{'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}` with a cross-validation F1 Score of 0.5283.\n",
        "\n",
        "## Final Conclusion\n",
        "\n",
        "### Understanding of the Dataset\n",
        "- The project started with a multi-class wine quality dataset, which was successfully transformed into a more actionable binary classification problem to predict 'Good' vs. 'Bad' wine based on chemical properties.\n",
        "\n",
        "### Important Observations from EDA\n",
        "- A critical finding was the severe class imbalance in the original 'quality' scores, which necessitated techniques like SMOTE to ensure fair model training.\n",
        "\n",
        "### Best Performing Model\n",
        "- The **Random Forest Classifier** demonstrated the strongest performance in the initial comparison (F1 Score: 0.7126). While hyperparameter tuning improved the SVM's cross-validation score, the Random Forest's test F1 score remained higher, indicating its strong generalization capabilities for this dataset.\n",
        "\n",
        "### What was learned from this project?\n",
        "- **Data Preprocessing is Key:** The importance of thorough data inspection, handling class imbalance (SMOTE), and feature scaling for optimal model performance.\n",
        "- **Problem Transformation:** Converting a multi-class problem to binary can significantly enhance real-world applicability and interpretability.\n",
        "- **Model Selection & Metrics:** The necessity of evaluating multiple models and choosing appropriate metrics (like F1 Score) for imbalanced datasets.\n",
        "- **Robust ML Practices:** The value of using pipelines for streamlined workflows and hyperparameter tuning for maximizing model potential and generalizability.\n",
        "\n",
        "### How this project is similar to real-world Machine Learning applications\n",
        "- This project emulates real-world scenarios by starting with a raw dataset, defining a practical problem, addressing data quality issues (imbalance), exploring multiple modeling approaches, and systematically evaluating their performance using robust techniques like train-test splits, cross-validation, and pipelines. The iterative nature of model selection and tuning reflects typical ML development lifecycles in industry."
      ]
    }
  ]
}